{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pathlib import PurePath\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "Data preprocessing refers to the process of cleaning, transforming, and preparing raw data before conducting data analysis. The following are several important functions of data preprocessing:\n",
    "\n",
    "- Data cleaning: Data preprocessing involves identifying and handling outliers, missing values, duplicate values, or erroneous values in the data. Cleaning the data helps ensure data quality and accuracy, avoiding adverse effects on the analysis results.\n",
    "- Data transformation: Through data preprocessing, data can be transformed into formats or representations suitable for specific analysis methods. For example, transformation operations such as feature scaling, standardization, normalization, or discretization can be performed to obtain better results in subsequent analysis.\n",
    "- Data integration: When data comes from different sources or different data tables, data preprocessing can help integrate them into a consistent dataset for analysis and modeling purposes.\n",
    "- Handling missing values: Missing values are common in real-world data. Data preprocessing can utilize imputation methods such as mean imputation, median imputation, or regression imputation to handle missing values, thereby reducing data loss and analysis bias.\n",
    "- Tagging as 'internal testing period': the internal testing period refers to a short interval between informal testing and formal testing, during which the releases are labeled as NA and cannot be filled.\n",
    "\n",
    "By performing data preprocessing, data quality, accuracy, and consistency can be improved, providing a more reliable foundation for subsequent quantitative and qualitative analysis tasks. It helps reduce analysis errors, enhance model performance, and ensure meaningful conclusions and insights are derived from the data.\n",
    "\n",
    "The data being processed in this case primarily consists of two parts: true release data and report event data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing for True Release Data\n",
    "\n",
    "In order to facilitate subsequent analysis tasks, the real release dose data processing in this project involves the following steps:\n",
    "\n",
    "1. Detect and interpolate missing values in methane flow rate data using adjacent time points\n",
    "2. Organize the data daily, spanning from 00:00:00 to 23:59:59 UTC\n",
    "3. Mark internal testing periods as N/A\n",
    "4. tag all true release: \n",
    "    - \"0\" indicates that the original release data\n",
    "    - \"1\" indicates that the data represents the interval testing, and the True Release rate (kg/h) is set as NA.\n",
    "    - \"2\" indicates that the data with True Release rate (kg/h) as NA has been replaced with linearly interpolated data, usually for the middle time of each day.\n",
    "    - \"3\" indicates nightly padding, where the start and end of the raw data are extended to the current 00:00:00 and 23:59:59, and the True Release rate (kg/h) is set as 0.\n",
    "\n",
    "These steps are performed to ensure the completeness and continuity of the real release dose data for further analysis. By filling in missing values and creating a continuous timeline, we can facilitate accurate analysis and interpretation of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_10.csv\n",
      "\tOrigin data length:  19801\n",
      "\tOrigin NA number: 1697, not NA number: 18104\n",
      "\tAfter linear interpolation, NA number: 1667, not NA number: 18134\n",
      "10_11.csv\n",
      "\tOrigin data length:  26940\n",
      "\tOrigin NA number: 951, not NA number: 25989\n",
      "\tAfter linear interpolation, NA number: 793, not NA number: 26147\n",
      "10_12.csv\n",
      "\tOrigin data length:  77101\n",
      "\tOrigin NA number: 56714, not NA number: 20387\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 77101\n",
      "10_13.csv\n",
      "\tOrigin data length:  6460\n",
      "\tOrigin NA number: 2461, not NA number: 3999\n",
      "\tAfter linear interpolation, NA number: 2449, not NA number: 4011\n",
      "10_14.csv\n",
      "\tOrigin data length:  2081\n",
      "\tOrigin NA number: 21, not NA number: 2060\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 2081\n",
      "10_17.csv\n",
      "\tOrigin data length:  10903\n",
      "\tOrigin NA number: 2418, not NA number: 8485\n",
      "\tAfter linear interpolation, NA number: 2408, not NA number: 8495\n",
      "10_18.csv\n",
      "\tOrigin data length:  4201\n",
      "\tOrigin NA number: 1277, not NA number: 2924\n",
      "\tAfter linear interpolation, NA number: 1274, not NA number: 2927\n",
      "10_19.csv\n",
      "\tOrigin data length:  12113\n",
      "\tOrigin NA number: 4976, not NA number: 7137\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 12113\n",
      "10_24.csv\n",
      "\tOrigin data length:  29144\n",
      "\tOrigin NA number: 10, not NA number: 29134\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 29144\n",
      "10_25.csv\n",
      "\tOrigin data length:  21261\n",
      "\tOrigin NA number: 30, not NA number: 21231\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 21261\n",
      "10_26.csv\n",
      "\tOrigin data length:  23948\n",
      "\tOrigin NA number: 1749, not NA number: 22199\n",
      "\tAfter linear interpolation, NA number: 1707, not NA number: 22241\n",
      "10_27.csv\n",
      "\tOrigin data length:  19702\n",
      "\tOrigin NA number: 533, not NA number: 19169\n",
      "\tAfter linear interpolation, NA number: 531, not NA number: 19171\n",
      "10_28.csv\n",
      "\tOrigin data length:  24684\n",
      "\tOrigin NA number: 21, not NA number: 24663\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 24684\n",
      "10_29.csv\n",
      "\tOrigin data length:  21109\n",
      "\tOrigin NA number: 23, not NA number: 21086\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 21109\n",
      "10_30.csv\n",
      "\tOrigin data length:  3340\n",
      "\tOrigin NA number: 10, not NA number: 3330\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 3340\n",
      "10_31.csv\n",
      "\tOrigin data length:  18697\n",
      "\tOrigin NA number: 48, not NA number: 18649\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 18697\n",
      "11_01.csv\n",
      "\tOrigin data length:  7201\n",
      "\tOrigin NA number: 1031, not NA number: 6170\n",
      "\tAfter linear interpolation, NA number: 996, not NA number: 6205\n",
      "11_02.csv\n",
      "\tOrigin data length:  8821\n",
      "\tOrigin NA number: 1622, not NA number: 7199\n",
      "\tAfter linear interpolation, NA number: 1622, not NA number: 7199\n",
      "11_03.csv\n",
      "\tOrigin data length:  5959\n",
      "\tOrigin NA number: 2174, not NA number: 3785\n",
      "\tAfter linear interpolation, NA number: 2171, not NA number: 3788\n",
      "11_04.csv\n",
      "\tOrigin data length:  18745\n",
      "\tOrigin NA number: 7, not NA number: 18738\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 18745\n",
      "11_07.csv\n",
      "\tOrigin data length:  20651\n",
      "\tOrigin NA number: 0, not NA number: 20651\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 20651\n",
      "11_08.csv\n",
      "\tOrigin data length:  84815\n",
      "\tOrigin NA number: 60792, not NA number: 24023\n",
      "\tAfter linear interpolation, NA number: 1, not NA number: 84814\n",
      "11_10.csv\n",
      "\tOrigin data length:  18661\n",
      "\tOrigin NA number: 989, not NA number: 17672\n",
      "\tAfter linear interpolation, NA number: 977, not NA number: 17684\n",
      "11_11.csv\n",
      "\tOrigin data length:  24841\n",
      "\tOrigin NA number: 679, not NA number: 24162\n",
      "\tAfter linear interpolation, NA number: 645, not NA number: 24196\n",
      "11_14.csv\n",
      "\tOrigin data length:  21961\n",
      "\tOrigin NA number: 1109, not NA number: 20852\n",
      "\tAfter linear interpolation, NA number: 1098, not NA number: 20863\n",
      "11_15.csv\n",
      "\tOrigin data length:  24121\n",
      "\tOrigin NA number: 2220, not NA number: 21901\n",
      "\tAfter linear interpolation, NA number: 2216, not NA number: 21905\n",
      "11_16.csv\n",
      "\tOrigin data length:  21481\n",
      "\tOrigin NA number: 3362, not NA number: 18119\n",
      "\tAfter linear interpolation, NA number: 3362, not NA number: 18119\n",
      "11_17.csv\n",
      "\tOrigin data length:  23281\n",
      "\tOrigin NA number: 2863, not NA number: 20418\n",
      "\tAfter linear interpolation, NA number: 2861, not NA number: 20420\n",
      "11_18.csv\n",
      "\tOrigin data length:  15901\n",
      "\tOrigin NA number: 2654, not NA number: 13247\n",
      "\tAfter linear interpolation, NA number: 2653, not NA number: 13248\n",
      "11_21.csv\n",
      "\tOrigin data length:  28066\n",
      "\tOrigin NA number: 1753, not NA number: 26313\n",
      "\tAfter linear interpolation, NA number: 1753, not NA number: 26313\n",
      "11_22.csv\n",
      "\tOrigin data length:  23581\n",
      "\tOrigin NA number: 2467, not NA number: 21114\n",
      "\tAfter linear interpolation, NA number: 2460, not NA number: 21121\n",
      "11_23.csv\n",
      "\tOrigin data length:  23341\n",
      "\tOrigin NA number: 2159, not NA number: 21182\n",
      "\tAfter linear interpolation, NA number: 2143, not NA number: 21198\n",
      "11_28.csv\n",
      "\tOrigin data length:  12301\n",
      "\tOrigin NA number: 2370, not NA number: 9931\n",
      "\tAfter linear interpolation, NA number: 2362, not NA number: 9939\n",
      "11_29.csv\n",
      "\tOrigin data length:  6961\n",
      "\tOrigin NA number: 2483, not NA number: 4478\n",
      "\tAfter linear interpolation, NA number: 2483, not NA number: 4478\n",
      "11_30.csv\n",
      "\tOrigin data length:  1801\n",
      "\tOrigin NA number: 0, not NA number: 1801\n",
      "\tAfter linear interpolation, NA number: 0, not NA number: 1801\n",
      "total processed NA number:  123041\n"
     ]
    }
   ],
   "source": [
    "# Set the data path\n",
    "data_path = PurePath(\"../../assets/Raw_Data_Per_Day\")\n",
    "\n",
    "# Get all the files of true release data and sort them\n",
    "filenames = []\n",
    "for name in os.listdir(data_path):\n",
    "    filenames.append(name)\n",
    "\n",
    "# Function used to sort file names\n",
    "def filenames_sort_map(name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        name: str, file name\n",
    "        \n",
    "    Returns:\n",
    "        str, sorted file name\n",
    "    \"\"\"\n",
    "    s, _ = os.path.splitext(name)\n",
    "    m, d = s.split(\"_\")\n",
    "    return \"%02d-%02d\" %(int(m), int(d))\n",
    "\n",
    "\n",
    "filenames = sorted(filenames, key=filenames_sort_map)\n",
    "precess_na_number = 0\n",
    "\n",
    "# 1. Handling missing values in the middle of each day\n",
    "valid_true_data = None\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    file_path = PurePath(data_path, filename)\n",
    "    data_t = pd.read_csv(file_path)\n",
    "    t_s = data_t.loc[0, \"datetime_utc\"]\n",
    "    t_e = data_t.loc[data_t.index[-1], \"datetime_utc\"]\n",
    "\n",
    "    # Check if time is continuous\n",
    "    assert (pd.Timestamp(t_e) - pd.Timestamp(t_s)) / pd.Timedelta(1, \"s\") == data_t.shape[0] - 1, print(t_e, t_s, data_t.shape[0])\n",
    "    \n",
    "    # add tag column for data_t, default value is 0, which means original data, \n",
    "    # 1 means internal testing period, 2 means linear interpolation, 3 means nightly padding\n",
    "    data_t[\"tag\"] = 0\n",
    "    print(\"\\tOrigin data length: \", data_t.shape[0])\n",
    "    print(\"\\tOrigin NA number: {}, not NA number: {}\".format(data_t[\"methane_kgh\"].isna().sum(), data_t[\"methane_kgh\"].notna().sum()))\n",
    "    prev_na_number = data_t[\"methane_kgh\"].isna().sum()\n",
    "    prev_notna_number = data_t[\"methane_kgh\"].notna().sum()\n",
    "\n",
    "\n",
    "    # For the middle of each day (from the first non-NA to the last non-NA), use linear interpolation to fill in missing values, and mark the filled NA values as 2\n",
    "    not_na_index = data_t.loc[data_t[\"methane_kgh\"].notna(), :].index\n",
    "    start_not_na = not_na_index[0]\n",
    "    end_not_na = not_na_index[-1]\n",
    "    mid_idx = np.zeros(data_t.shape[0])\n",
    "    mid_idx[start_not_na:end_not_na+1] = 1\n",
    "    data_t.loc[mid_idx & (data_t[\"methane_kgh\"].isna()), \"tag\"] = 2 # 2 means linear approximation\n",
    "    data_t.loc[start_not_na:end_not_na, \"methane_kgh\"] = data_t.loc[start_not_na:end_not_na, \"methane_kgh\"].interpolate()\n",
    "    precess_na_number += (data_t[\"tag\"] == 2).sum()\n",
    "\n",
    "    print(\"\\tAfter linear interpolation, NA number: {}, not NA number: {}\".format(\n",
    "        data_t[\"methane_kgh\"].isna().sum(), \n",
    "        data_t[\"methane_kgh\"].notna().sum()))\n",
    "\n",
    "    data_t = data_t.rename(columns={\"datetime_utc\": \"Datetime (UTC)\", \"methane_kgh\": \"Release Rate (kg/h)\"})\n",
    "    data_t = data_t[[\"Datetime (UTC)\", \"Release Rate (kg/h)\", \"tag\"]]\n",
    "\n",
    "\n",
    "    # Merge the data for each day together\n",
    "    if valid_true_data is None:\n",
    "        valid_true_data = data_t\n",
    "    else:\n",
    "        valid_true_data = pd.concat([valid_true_data, data_t], axis=0)\n",
    "    \n",
    "print(\"total processed NA number: \", precess_na_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4492800/4492800 [00:33<00:00, 134477.82it/s]\n",
      "100%|██████████| 713975/713975 [00:19<00:00, 37209.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After continuous processing, interpolation number is 123041, nightly padding number is 3819457, origin number is 550302\n"
     ]
    }
   ],
   "source": [
    "# 2. Handling missing values at the beginning and end of each day\n",
    "# Fill in 0 values for the beginning and end of each day, and ensure data continuity, marked padding data as 3\n",
    "valid_true_data[\"Datetime (UTC)\"] = pd.to_datetime(valid_true_data[\"Datetime (UTC)\"])\n",
    "valid_true_data.sort_values(by=\"Datetime (UTC)\", inplace=True)\n",
    "valid_true_data.reset_index(drop=True, inplace=True)\n",
    "timeInterval = pd.Timestamp(valid_true_data.loc[valid_true_data.shape[0]-1, \"Datetime (UTC)\"].date()) + pd.Timedelta(1, \"d\") - pd.Timestamp(valid_true_data.loc[0, \"Datetime (UTC)\"].date())\n",
    "data_len = timeInterval / pd.Timedelta(1, \"s\")\n",
    "new_valid_true_data = pd.DataFrame(columns=[\"Datetime (UTC)\", \"True Release Rate (kg/h)\"])\n",
    "start_time = pd.Timestamp(valid_true_data.loc[0, \"Datetime (UTC)\"].date())\n",
    "\n",
    "new_datetime = []\n",
    "for i in tqdm(range(0, int(data_len))):\n",
    "    new_datetime.append(start_time + pd.Timedelta(i, \"s\"))\n",
    "new_valid_true_data[\"Datetime (UTC)\"] = new_datetime\n",
    "new_valid_true_data[\"True Release Rate (kg/h)\"] = 0\n",
    "new_valid_true_data[\"tag\"] = 3 # 3 means night padding\n",
    "\n",
    "true_data_indexes = []\n",
    "for i in tqdm(range(valid_true_data.shape[0])):\n",
    "    if pd.notna(valid_true_data.loc[i, \"Release Rate (kg/h)\"]):\n",
    "        true_data_indexes.append(\n",
    "            int((valid_true_data.loc[i, \"Datetime (UTC)\"] - start_time) / pd.Timedelta(1, \"s\"))\n",
    "        )\n",
    "new_valid_true_data.loc[true_data_indexes, \"True Release Rate (kg/h)\"] = valid_true_data.loc[valid_true_data[\"Release Rate (kg/h)\"].notna(), \"Release Rate (kg/h)\"].tolist()\n",
    "# merge tag column\n",
    "new_valid_true_data.loc[true_data_indexes, \"tag\"] = valid_true_data.loc[valid_true_data[\"Release Rate (kg/h)\"].notna(), \"tag\"].tolist()\n",
    "\n",
    "print(\"After continuous processing, interpolation number is {}, nightly padding number is {}, origin number is {}\".format(\n",
    "    new_valid_true_data.loc[new_valid_true_data[\"tag\"] == 2, \"tag\"].shape[0],\n",
    "    new_valid_true_data.loc[new_valid_true_data[\"tag\"] == 3, \"tag\"].shape[0],\n",
    "    new_valid_true_data.loc[new_valid_true_data[\"tag\"] == 0, \"tag\"].shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Internal testing period:\n",
      "Internal testing period: start: 2022-10-10 16:42:29, end: 2022-10-10 17:00:00     Ignored NA number: 1052\n",
      "Internal testing period: start: 2022-10-10 21:36:00, end: 2022-10-11 00:11:29     Ignored NA number: 9330\n",
      "Internal testing period: start: 2022-10-11 16:43:13, end: 2022-10-11 17:16:17     Ignored NA number: 1985\n",
      "Internal testing period: start: 2022-10-11 21:16:36, end: 2022-10-12 00:02:42     Ignored NA number: 9967\n",
      "Internal testing period: start: 2022-10-12 15:41:07, end: 2022-10-12 17:15:56     Ignored NA number: 5690\n",
      "Internal testing period: start: 2022-10-12 21:00:00, end: 2022-10-12 23:00:00     Ignored NA number: 7201\n",
      "Internal testing period: start: 2022-10-13 17:50:11, end: 2022-10-13 20:40:00     Ignored NA number: 10190\n",
      "Internal testing period: start: 2022-10-13 21:08:00, end: 2022-10-14 00:26:01     Ignored NA number: 11882\n",
      "Internal testing period: start: 2022-10-14 15:16:11, end: 2022-10-14 16:25:00     Ignored NA number: 4130\n",
      "Internal testing period: start: 2022-10-17 16:41:11, end: 2022-10-17 17:54:00     Ignored NA number: 4370\n",
      "Internal testing period: start: 2022-10-17 18:10:00, end: 2022-10-17 21:41:45     Ignored NA number: 12706\n",
      "Internal testing period: start: 2022-10-18 16:21:07, end: 2022-10-18 17:45:00     Ignored NA number: 5034\n",
      "Internal testing period: start: 2022-10-18 18:00:00, end: 2022-10-18 20:29:20     Ignored NA number: 8961\n",
      "Internal testing period: start: 2022-10-24 14:50:07, end: 2022-10-24 16:40:00     Ignored NA number: 6594\n",
      "Internal testing period: start: 2022-10-24 19:50:00, end: 2022-10-24 22:55:52     Ignored NA number: 11153\n",
      "Internal testing period: start: 2022-10-25 15:17:29, end: 2022-10-25 16:30:00     Ignored NA number: 4352\n",
      "Internal testing period: start: 2022-10-25 20:50:00, end: 2022-10-25 21:43:22     Ignored NA number: 3203\n",
      "Internal testing period: start: 2022-10-26 16:35:08, end: 2022-10-26 16:39:00     Ignored NA number: 233\n",
      "Internal testing period: start: 2022-10-26 20:40:55, end: 2022-10-26 22:29:28     Ignored NA number: 6514\n",
      "Internal testing period: start: 2022-10-27 15:58:51, end: 2022-10-27 16:35:00     Ignored NA number: 2170\n",
      "Internal testing period: start: 2022-10-27 20:14:15, end: 2022-10-27 21:18:23     Ignored NA number: 3849\n",
      "Internal testing period: start: 2022-10-28 15:46:57, end: 2022-10-28 16:40:00     Ignored NA number: 3184\n",
      "Internal testing period: start: 2022-10-28 21:07:20, end: 2022-10-28 21:48:07     Ignored NA number: 2448\n",
      "Internal testing period: start: 2022-10-29 15:31:04, end: 2022-10-29 16:25:01     Ignored NA number: 3238\n",
      "Internal testing period: start: 2022-10-29 21:11:08, end: 2022-10-29 21:26:20     Ignored NA number: 913\n",
      "Internal testing period: start: 2022-10-31 16:13:24, end: 2022-10-31 17:02:00     Ignored NA number: 2917\n",
      "Internal testing period: start: 2022-10-31 21:15:00, end: 2022-10-31 21:19:20     Ignored NA number: 261\n",
      "Internal testing period: start: 2022-11-01 17:31:36, end: 2022-11-01 18:00:00     Ignored NA number: 1705\n",
      "Internal testing period: start: 2022-11-01 19:05:00, end: 2022-11-01 19:13:00     Ignored NA number: 481\n",
      "Internal testing period: start: 2022-11-02 16:20:02, end: 2022-11-02 16:38:20     Ignored NA number: 1099\n",
      "Internal testing period: start: 2022-11-02 18:10:00, end: 2022-11-02 18:56:07     Ignored NA number: 2768\n",
      "Internal testing period: start: 2022-11-03 17:36:11, end: 2022-11-03 17:45:00     Ignored NA number: 530\n",
      "Internal testing period: start: 2022-11-03 18:10:00, end: 2022-11-03 18:56:07     Ignored NA number: 2768\n",
      "Internal testing period: start: 2022-11-04 15:38:09, end: 2022-11-04 16:43:19     Ignored NA number: 3911\n",
      "Internal testing period: start: 2022-11-04 20:32:00, end: 2022-11-04 20:50:49     Ignored NA number: 1130\n",
      "Internal testing period: start: 2022-11-07 16:34:50, end: 2022-11-07 17:59:44     Ignored NA number: 5095\n",
      "Internal testing period: start: 2022-11-07 22:09:13, end: 2022-11-07 22:13:54     Ignored NA number: 282\n",
      "Internal testing period: start: 2022-11-08 17:52:09, end: 2022-11-08 18:09:24     Ignored NA number: 1036\n",
      "Internal testing period: start: 2022-11-08 23:55:03, end: 2022-11-08 23:59:09     Ignored NA number: 247\n",
      "Internal testing period: start: 2022-11-10 17:05:08, end: 2022-11-10 17:49:21     Ignored NA number: 2654\n",
      "Internal testing period: start: 2022-11-10 22:05:02, end: 2022-11-10 22:08:55     Ignored NA number: 234\n",
      "Internal testing period: start: 2022-11-11 17:08:39, end: 2022-11-11 17:43:45     Ignored NA number: 2107\n",
      "Internal testing period: start: 2022-11-11 23:42:00, end: 2022-11-11 23:45:52     Ignored NA number: 233\n",
      "Internal testing period: start: 2022-11-14 16:58:15, end: 2022-11-14 17:25:00     Ignored NA number: 1606\n",
      "Internal testing period: start: 2022-11-14 22:36:00, end: 2022-11-14 22:38:18     Ignored NA number: 139\n",
      "Internal testing period: start: 2022-11-15 16:16:52, end: 2022-11-15 16:25:00     Ignored NA number: 489\n",
      "Internal testing period: start: 2022-11-15 22:12:00, end: 2022-11-15 22:15:24     Ignored NA number: 205\n",
      "Internal testing period: start: 2022-11-16 16:01:16, end: 2022-11-16 16:02:00     Ignored NA number: 45\n",
      "Internal testing period: start: 2022-11-17 22:03:00, end: 2022-11-17 22:04:56     Ignored NA number: 117\n",
      "Internal testing period: start: 2022-11-18 14:59:13, end: 2022-11-18 15:00:00     Ignored NA number: 48\n",
      "Internal testing period: start: 2022-11-18 18:30:00, end: 2022-11-18 18:35:55     Ignored NA number: 356\n",
      "Internal testing period: start: 2022-11-21 15:19:13, end: 2022-11-21 15:35:00     Ignored NA number: 948\n",
      "Internal testing period: start: 2022-11-21 22:22:07, end: 2022-11-21 22:37:40     Ignored NA number: 934\n",
      "Internal testing period: start: 2022-11-22 16:36:00, end: 2022-11-22 16:40:00     Ignored NA number: 241\n",
      "Internal testing period: start: 2022-11-22 22:18:08, end: 2022-11-22 22:18:38     Ignored NA number: 31\n",
      "Internal testing period: start: 2022-11-23 16:18:43, end: 2022-11-23 16:28:00     Ignored NA number: 558\n",
      "Internal testing period: start: 2022-11-23 22:02:04, end: 2022-11-23 22:10:21     Ignored NA number: 498\n",
      "Internal testing period: start: 2022-11-28 16:24:19, end: 2022-11-28 16:30:00     Ignored NA number: 342\n",
      "Internal testing period: start: 2022-11-29 17:25:23, end: 2022-11-29 17:29:21     Ignored NA number: 239\n",
      "176603 number of points have been filtered\n"
     ]
    }
   ],
   "source": [
    "# Tagging as 'internal testing period', marked as 1\n",
    "testing_period_data_path = PurePath(\"../../assets/Daily Test Cycle.xlsx\")\n",
    "testing_period_data = pd.read_excel(testing_period_data_path)\n",
    "testing_period_data.dropna(axis=0, how=\"all\", inplace=True)\n",
    "\n",
    "# Convert the time format of the testing period data to the same format as the data\n",
    "columns = [\"informal testing start\",\n",
    "           \"informal testing end\",\n",
    "           \"official testing start\",\n",
    "           \"official testing end\"]\n",
    "for column in columns:\n",
    "    testing_period_data[column] = pd.to_datetime(testing_period_data[column])\n",
    "\n",
    "\n",
    "\n",
    "# extract the internal testing period\n",
    "testing_period = []\n",
    "for i in range(testing_period_data.shape[0]):\n",
    "    informal_start = testing_period_data.loc[i, \"informal testing start\"]\n",
    "    informal_end = testing_period_data.loc[i, \"informal testing end\"]\n",
    "    official_start = testing_period_data.loc[i, \"official testing start\"]\n",
    "    official_end = testing_period_data.loc[i, \"official testing end\"]\n",
    "\n",
    "    if official_start > informal_start:\n",
    "        testing_period.append([informal_start, official_start])\n",
    "    if official_end < informal_end:\n",
    "        testing_period.append([official_end, informal_end])\n",
    "print(\"Processing Internal testing period:\" )\n",
    "for period in testing_period:\n",
    "    start = period[0]\n",
    "    end = period[1]\n",
    "    print(\"Internal testing period: start: {}, end: {}\".format(start, end), end=\"     \")\n",
    "    na_number = new_valid_true_data[\"True Release Rate (kg/h)\"].isna().sum()\n",
    "    new_valid_true_data.loc[(new_valid_true_data[\"Datetime (UTC)\"] >= start) & (new_valid_true_data[\"Datetime (UTC)\"] <= end), \"tag\"] = 1\n",
    "    new_valid_true_data.loc[(new_valid_true_data[\"Datetime (UTC)\"] >= start) & (new_valid_true_data[\"Datetime (UTC)\"] <= end), \"True Release Rate (kg/h)\"] = np.nan\n",
    "    print(\"Ignored NA number: {}\".format(new_valid_true_data[\"True Release Rate (kg/h)\"].isna().sum() - na_number))\n",
    "\n",
    "print(\"{} number of points have been filtered\".format(new_valid_true_data.loc[new_valid_true_data[\"tag\"] == 1, \"tag\"].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  2022-10-10\n",
      "\tNumber of Original data: 16545\n",
      "\tNumber of Interpolation data: 14\n",
      "\tNumber of Nightly padding data: 60149\n",
      "\tNumber of Ignored data: 9692\n",
      "Date:  2022-10-11\n",
      "\tNumber of Original data: 14299\n",
      "\tNumber of Interpolation data: 119\n",
      "\tNumber of Nightly padding data: 59503\n",
      "\tNumber of Ignored data: 12479\n",
      "Date:  2022-10-12\n",
      "\tNumber of Original data: 13430\n",
      "\tNumber of Interpolation data: 56317\n",
      "\tNumber of Nightly padding data: 3599\n",
      "\tNumber of Ignored data: 13054\n",
      "Date:  2022-10-13\n",
      "\tNumber of Original data: 1670\n",
      "\tNumber of Interpolation data: 9\n",
      "\tNumber of Nightly padding data: 64211\n",
      "\tNumber of Ignored data: 20510\n",
      "Date:  2022-10-14\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-15\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-10-16\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-10-17\n",
      "\tNumber of Original data: 958\n",
      "\tNumber of Interpolation data: 1\n",
      "\tNumber of Nightly padding data: 68365\n",
      "\tNumber of Ignored data: 17076\n",
      "Date:  2022-10-18\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-19\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-20\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-21\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-22\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 0\n",
      "\tNumber of Ignored data: 86400\n",
      "Date:  2022-10-23\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-10-24\n",
      "\tNumber of Original data: 11391\n",
      "\tNumber of Interpolation data: 8\n",
      "\tNumber of Nightly padding data: 57254\n",
      "\tNumber of Ignored data: 17747\n",
      "Date:  2022-10-25\n",
      "\tNumber of Original data: 15581\n",
      "\tNumber of Interpolation data: 18\n",
      "\tNumber of Nightly padding data: 63246\n",
      "\tNumber of Ignored data: 7555\n",
      "Date:  2022-10-26\n",
      "\tNumber of Original data: 15509\n",
      "\tNumber of Interpolation data: 6\n",
      "\tNumber of Nightly padding data: 64138\n",
      "\tNumber of Ignored data: 6747\n",
      "Date:  2022-10-27\n",
      "\tNumber of Original data: 13154\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 67227\n",
      "\tNumber of Ignored data: 6019\n",
      "Date:  2022-10-28\n",
      "\tNumber of Original data: 19034\n",
      "\tNumber of Interpolation data: 18\n",
      "\tNumber of Nightly padding data: 61716\n",
      "\tNumber of Ignored data: 5632\n",
      "Date:  2022-10-29\n",
      "\tNumber of Original data: 17153\n",
      "\tNumber of Interpolation data: 13\n",
      "\tNumber of Nightly padding data: 65083\n",
      "\tNumber of Ignored data: 4151\n",
      "Date:  2022-10-30\n",
      "\tNumber of Original data: 3330\n",
      "\tNumber of Interpolation data: 10\n",
      "\tNumber of Nightly padding data: 83060\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-10-31\n",
      "\tNumber of Original data: 15479\n",
      "\tNumber of Interpolation data: 40\n",
      "\tNumber of Nightly padding data: 67703\n",
      "\tNumber of Ignored data: 3178\n",
      "Date:  2022-11-01\n",
      "\tNumber of Original data: 3993\n",
      "\tNumber of Interpolation data: 26\n",
      "\tNumber of Nightly padding data: 80195\n",
      "\tNumber of Ignored data: 2186\n",
      "Date:  2022-11-02\n",
      "\tNumber of Original data: 5499\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 77034\n",
      "\tNumber of Ignored data: 3867\n",
      "Date:  2022-11-03\n",
      "\tNumber of Original data: 1497\n",
      "\tNumber of Interpolation data: 2\n",
      "\tNumber of Nightly padding data: 81603\n",
      "\tNumber of Ignored data: 3298\n",
      "Date:  2022-11-04\n",
      "\tNumber of Original data: 13719\n",
      "\tNumber of Interpolation data: 1\n",
      "\tNumber of Nightly padding data: 67639\n",
      "\tNumber of Ignored data: 5041\n",
      "Date:  2022-11-05\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-06\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-07\n",
      "\tNumber of Original data: 15274\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 65749\n",
      "\tNumber of Ignored data: 5377\n",
      "Date:  2022-11-08\n",
      "\tNumber of Original data: 22769\n",
      "\tNumber of Interpolation data: 60772\n",
      "\tNumber of Nightly padding data: 1576\n",
      "\tNumber of Ignored data: 1283\n",
      "Date:  2022-11-09\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-10\n",
      "\tNumber of Original data: 15688\n",
      "\tNumber of Interpolation data: 11\n",
      "\tNumber of Nightly padding data: 67813\n",
      "\tNumber of Ignored data: 2888\n",
      "Date:  2022-11-11\n",
      "\tNumber of Original data: 21830\n",
      "\tNumber of Interpolation data: 26\n",
      "\tNumber of Nightly padding data: 62204\n",
      "\tNumber of Ignored data: 2340\n",
      "Date:  2022-11-12\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-13\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-14\n",
      "\tNumber of Original data: 19113\n",
      "\tNumber of Interpolation data: 5\n",
      "\tNumber of Nightly padding data: 65537\n",
      "\tNumber of Ignored data: 1745\n",
      "Date:  2022-11-15\n",
      "\tNumber of Original data: 21211\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 64495\n",
      "\tNumber of Ignored data: 694\n",
      "Date:  2022-11-16\n",
      "\tNumber of Original data: 18074\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 68281\n",
      "\tNumber of Ignored data: 45\n",
      "Date:  2022-11-17\n",
      "\tNumber of Original data: 20303\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 65980\n",
      "\tNumber of Ignored data: 117\n",
      "Date:  2022-11-18\n",
      "\tNumber of Original data: 12844\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 73152\n",
      "\tNumber of Ignored data: 404\n",
      "Date:  2022-11-19\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-20\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-21\n",
      "\tNumber of Original data: 24431\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 60087\n",
      "\tNumber of Ignored data: 1882\n",
      "Date:  2022-11-22\n",
      "\tNumber of Original data: 20842\n",
      "\tNumber of Interpolation data: 7\n",
      "\tNumber of Nightly padding data: 65279\n",
      "\tNumber of Ignored data: 272\n",
      "Date:  2022-11-23\n",
      "\tNumber of Original data: 20126\n",
      "\tNumber of Interpolation data: 16\n",
      "\tNumber of Nightly padding data: 65202\n",
      "\tNumber of Ignored data: 1056\n",
      "Date:  2022-11-24\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-25\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-26\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-27\n",
      "\tNumber of Original data: 0\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 86400\n",
      "\tNumber of Ignored data: 0\n",
      "Date:  2022-11-28\n",
      "\tNumber of Original data: 9592\n",
      "\tNumber of Interpolation data: 8\n",
      "\tNumber of Nightly padding data: 76458\n",
      "\tNumber of Ignored data: 342\n",
      "Date:  2022-11-29\n",
      "\tNumber of Original data: 4239\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 81922\n",
      "\tNumber of Ignored data: 239\n",
      "Date:  2022-11-30\n",
      "\tNumber of Original data: 1801\n",
      "\tNumber of Interpolation data: 0\n",
      "\tNumber of Nightly padding data: 84599\n",
      "\tNumber of Ignored data: 0\n"
     ]
    }
   ],
   "source": [
    "# Save continuous data\n",
    "new_valid_true_data.to_csv(PurePath(\"../../assets/valid_true_data_pad.csv\"), index=False)\n",
    "\n",
    "# Save continuous data for each day\n",
    "for date in pd.date_range(start=pd.to_datetime(\"2022-10-10\").date(), end=pd.to_datetime(\"2022-11-30\").date(), freq=\"d\"):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    start_time = pd.Timestamp(date)\n",
    "    end_time = pd.Timestamp(date) + pd.Timedelta(1, \"d\")\n",
    "    data = new_valid_true_data.loc[(new_valid_true_data[\"Datetime (UTC)\"] >= start_time) & (new_valid_true_data[\"Datetime (UTC)\"] < end_time), :]\n",
    "    if date in [\n",
    "        pd.to_datetime(\"2022-10-14\").date(),\n",
    "        pd.to_datetime(\"2022-10-18\").date(),\n",
    "        pd.to_datetime(\"2022-10-19\").date(),\n",
    "        pd.to_datetime(\"2022-10-20\").date(),\n",
    "        pd.to_datetime(\"2022-10-21\").date(),\n",
    "        pd.to_datetime(\"2022-10-22\").date()\n",
    "    ]:\n",
    "        data.loc[:, \"tag\"] = 1\n",
    "        data.loc[:, \"True Release Rate (kg/h)\"] = np.nan\n",
    "\n",
    "    print(\"Date: \", date_str)\n",
    "    print(\"\\tNumber of Original data:\", (data[\"tag\"] == 0).sum())\n",
    "    print(\"\\tNumber of Interpolation data:\", (data[\"tag\"] == 2).sum())\n",
    "    print(\"\\tNumber of Nightly padding data:\", (data[\"tag\"] == 3).sum())\n",
    "    print(\"\\tNumber of Ignored data:\", (data[\"tag\"] == 1).sum())\n",
    "\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(PurePath(\"../../assets/valid_true_data_pad_daily\", \"valid_true_data_pad_daily_\" + date_str + \".csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing for Report Event Data\n",
    "\n",
    "For the report event data, the data preprocessing includes the following steps:\n",
    "\n",
    "1) Extraction of required data: The necessary data is parsed, including the event start time, end time, release dose, etc.\n",
    "2) Extraction of Missing Report Date: The missing report dates are extracted from the data.\n",
    "3) Extraction of Report Date: The report dates are extracted, which include the report start time and report end time.\n",
    "\n",
    "These steps are performed to preprocess the report event data, extracting the relevant information needed for further analysis. By extracting the necessary data and identifying missing report dates, we can ensure the completeness and accuracy of the dataset for subsequent analysis and interpretation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Extract valid event data for all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Qube data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:03,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  116\n",
      "Processing Oiler data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:03,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  148\n",
      "Processing Sensirion data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:01<00:02,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  51\n",
      "Processing Canary data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:02<00:02,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  46\n",
      "Processing Kuva data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:03<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  212\n",
      "Processing Ecoteco data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:04<00:01,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  532\n",
      "Processing Andium data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:04<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  164\n",
      "Processing Soofie data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Original events number:  4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract all sensor data \n",
    "DATA_TO_CSV = True\n",
    "\n",
    "# Set the root path and file names\n",
    "root = PurePath('../../assets/sensor_raw_data/')\n",
    "\n",
    "valid_event_df = {}\n",
    "\n",
    "names = ['Qube', 'Oiler', 'Sensirion', 'Canary','Kuva','Ecoteco','Andium', \"Soofie\"]\n",
    "files = ['0222_Qube_DataReportingTemplate_Continuous Monitoring.xlsx',\n",
    "         '0222_DataReportingTemplate_Continuous Monitoring _Oiler Equation.xlsx',\n",
    "         'DataReporting_Stanford_SensirionConnectedSolutions_20230228.xlsx',\n",
    "         'DataReportingTemplate_Project Canary_Continuous Monitoring_results.xlsx',\n",
    "         'DataReportingKUVA_Continuous Monitoring-Final.xlsx',\n",
    "         'DataReportingTemplate_Continuous  Monitoring_UTC_ECOTEC.xlsx',\n",
    "         'Andium_DataReporting_Continuous Monitoring_Final_revised.xlsx', \n",
    "         \"SOOFIE DataReportingTemplate_Continuous Monitoring updated 2022 06 13.xlsx\"]\n",
    "\n",
    "if DATA_TO_CSV:\n",
    "\n",
    "    # 2. Read the data and save to csv\n",
    "    for i in tqdm(range(len(names))):\n",
    "        print(\"Processing {} data\".format(names[i]))\n",
    "        sensor_name = names[i]\n",
    "        # Keep the valid columns\n",
    "        path = PurePath(root, files[i])\n",
    "        if names[i] == \"Soofie\":\n",
    "            lab_data = pd.read_excel(path, sheet_name='Survey Summary', parse_dates=['EmissionStartDateTime UTC','EmissionEndDateTime UTC'])\n",
    "            lab_data.rename(columns={'EmissionStartDateTime UTC':'EmissionStartDateTime', 'EmissionEndDateTime UTC':'EmissionEndDateTime'}, inplace=True)\n",
    "        else:\n",
    "            lab_data = pd.read_excel(path, sheet_name='Survey Summary', parse_dates=['EmissionStartDateTime','EmissionEndDateTime'])\n",
    "\n",
    "        if names[i] == \"Qube\":\n",
    "            lab_data[\"EmissionRateUpper\"] /= 1000\n",
    "            lab_data[\"EmissionRateLower\"] /= 1000\n",
    "\n",
    "        lab_data.dropna(subset=['EmissionStartDateTime','EmissionEndDateTime'], how='all', inplace=True)\n",
    "        valid_cols = ['EmissionStartDateTime', 'EmissionEndDateTime', 'EmissionRate', \"EmissionRateUpper\", \"EmissionRateLower\"]\n",
    "        lab_data = lab_data[valid_cols].copy()\n",
    "\n",
    "        lab_data.rename(columns={'EmissionRate':'Reported Release Rate (kg/h)'}, inplace=True)\n",
    "        lab_data.sort_values(by='EmissionStartDateTime', inplace=True)\n",
    "        if not os.path.exists(PurePath('../../assets/sensor_data')):\n",
    "            os.makedirs(PurePath('../../assets/sensor_data'))\n",
    "        print(\"\\t Original events number: \", lab_data.shape[0])\n",
    "        valid_event_df[names[i]] = lab_data\n",
    "        # Save to csv\n",
    "        # save_path = PurePath('../../assets/sensor_data/%s_validdata.csv'%names[i])\n",
    "        # lab_data.to_csv(save_path, index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Extract Offline Date\n",
    "only Kuva and Oiler have Offline Date, we can extract the date from **DataReportingKUVA_Continuous Monitoring-Final.xlsx** and **0222_DataReportingTemplate_Continuous Monitoring _Oiler Equation.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Date Reporting...\n",
      "For Kuva, offline time number is 51\n",
      "For Oiler, offline time number is 14\n"
     ]
    }
   ],
   "source": [
    "def get_offline_time(excel_file):\n",
    "    tmp_df = pd.read_excel(excel_file, sheet_name=\"Missing Data Reporting\")\n",
    "    tmp_df = tmp_df[[\"OfflineReportStartDateTime\", \"OfflineReportEndDateTime\"]]\n",
    "    tmp_df[\"OfflineReportStartDateTime\"] = pd.to_datetime(tmp_df[\"OfflineReportStartDateTime\"])\n",
    "    tmp_df[\"OfflineReportEndDateTime\"] = pd.to_datetime(tmp_df[\"OfflineReportEndDateTime\"])\n",
    "    tmp_df = tmp_df.dropna(subset=[\"OfflineReportStartDateTime\", \"OfflineReportEndDateTime\"], how=\"any\")\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "print(\"Missing Date Reporting...\")\n",
    "# For Kuva \n",
    "kuva_offline_time = get_offline_time(PurePath(root, \"DataReportingKUVA_Continuous Monitoring-Final.xlsx\"))\n",
    "kuva_offline_time.to_excel(PurePath(\"../../assets/sensor_data/Kuva_offline_data.xlsx\"), index=False, sheet_name=\"Missing Data Reporting\")\n",
    "print(\"For Kuva, offline time number is {}\".format(kuva_offline_time.shape[0]))\n",
    "\n",
    "# For Oiler\n",
    "oiler_offline_time = get_offline_time(PurePath(root, \"0222_DataReportingTemplate_Continuous Monitoring _Oiler Equation.xlsx\"))\n",
    "oiler_offline_time.to_excel(PurePath(\"../../assets/sensor_data/Oiler_offline_data.xlsx\"), index=False, sheet_name=\"Missing Data Reporting\")\n",
    "print(\"For Oiler, offline time number is {}\".format(oiler_offline_time.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Extract Online Report Days\n",
    "\n",
    "The Online Report Days also called Deploy time, which include the report start time and report end time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Date Reporting...\n",
      "Processing Qube data\n",
      "\t Online time is 2022-10-10 16:50:00->2022-11-23 22:12:00\n",
      "Processing Oiler data\n",
      "\t Online time is 2022-10-10 18:03:35->2022-11-03 18:56:33\n",
      "Processing Sensirion data\n",
      "\t Online time is 2022-10-10 16:51:00->2022-11-30 18:44:00\n",
      "Processing Canary data\n",
      "\t Online time is 2022-10-10 16:54:00->2022-11-29 18:33:00\n",
      "Processing Kuva data\n",
      "\t Online time is 2022-10-10 16:51:00->2022-11-23 22:03:00\n",
      "Processing Ecoteco data\n",
      "\t Online time is 2022-10-28 16:56:07->2022-11-28 17:45:23\n",
      "Processing Andium data\n",
      "\t Online time is 2022-10-10 16:50:46->2022-11-23 18:36:34\n",
      "Processing Soofie data\n",
      "\t Online time is 2022-10-10 00:00:00->2022-11-29 23:30:00\n"
     ]
    }
   ],
   "source": [
    "sensor_names = ['Qube', 'Oiler', 'Sensirion', 'Canary','Kuva','Ecoteco','Andium', \"Soofie\"]\n",
    "online_date = pd.DataFrame(columns=[\"Sensor\", \"Testing Beginning Date\", \"Testing Ending Date\"])\n",
    "release_start_time = pd.to_datetime(\"2022-10-10 00:00:00\")\n",
    "release_end_time = pd.to_datetime(\"2022-11-30 23:59:59\")\n",
    "\n",
    "print(\"Online Date Reporting...\")\n",
    "for i in range(len(sensor_names)):\n",
    "    print(\"Processing {} data\".format(sensor_names[i]))\n",
    "    tmp_df = valid_event_df[sensor_names[i]]\n",
    "    tmp_df[\"EmissionStartDateTime\"] = pd.to_datetime(tmp_df[\"EmissionStartDateTime\"])\n",
    "    tmp_df[\"EmissionEndDateTime\"] = pd.to_datetime(tmp_df[\"EmissionEndDateTime\"])\n",
    "    tmp_df = tmp_df[(tmp_df[\"EmissionStartDateTime\"] >= release_start_time) & (tmp_df[\"EmissionEndDateTime\"] <= release_end_time)]\n",
    "    tmp_df = tmp_df[[\"EmissionStartDateTime\", \"EmissionEndDateTime\"]]\n",
    "    print(\"\\t Online time is {}->{}\".format(tmp_df[\"EmissionStartDateTime\"].min(), tmp_df[\"EmissionEndDateTime\"].max()))\n",
    "    online_date.loc[i] = [sensor_names[i], tmp_df[\"EmissionStartDateTime\"].min(), tmp_df[\"EmissionEndDateTime\"].max()]\n",
    "\n",
    "online_date.to_excel(PurePath(\"../../assets/sensor_data/sensor_online_date.xlsx\"), index=False, sheet_name=\"online_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Team data process\n",
    "\n",
    "1. Set the report release rate for each event to N/A for (Andium/Ecoteco/Kuva);\n",
    "2. Definition of Events:\n",
    "\n",
    "    1) All sensors do not include SOOFIE\n",
    "        1) Positive Events: Release rate > 0 kg/hr or labeled as N/A\n",
    "        2) Negative Events: Release rate = 0 kg/hr\n",
    "        3) Events between two consecutive events are classified as negative \n",
    "    2) Only for SOOFIE\n",
    "        1) Positive Events: Release Rate > 0 kg/hr\n",
    "        2) Negative Events: Release Rate = 0 kg/hr\n",
    "        3) N/A Events: Release Rate is N/A\n",
    "\n",
    "3. Event filtering:\n",
    "    1) Canary: Short Stack Head Filtering, Exclude events that are not within the dates of October 31, 2022, and November 15 to November 30, 2022.\n",
    "    2) Kuva: Exclude events overlapping with Offline Report Dates\n",
    "    3) Oiler: Exclude events overlapping with Offline Report Dates\n",
    "    4) SOOFIE: Exclude events falling within the period of November 7, 2022, 22:30:00 to November 14, 2022, 16:30:00\n",
    "\n",
    "4. For each sensor, filter out all events that do not overlap with the sensor's Online Date\n",
    "5. Event Removal: \n",
    "    - Exclude reports on: 2022-10-14, 2022-10-18 to 2022-10-22\n",
    "    - Reason: Stanford internal testing period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andium\n",
      "Ecoteco\n",
      "Kuva\n",
      "Oiler\n",
      "Canary\n",
      "Qube\n",
      "Sensirion\n",
      "Soofie\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the report release rate for each event to N/A for (Andium/Ecoteco/Kuva);\n",
    "\n",
    "# For Andium\n",
    "andium_df = valid_event_df[\"Andium\"].copy()\n",
    "andium_df[\"Reported Release Rate (kg/h)\"] = np.nan\n",
    "andium_df[\"EmissionRateUpper\"] = np.nan\n",
    "andium_df[\"EmissionRateLower\"] = np.nan\n",
    "\n",
    "# For Ecoteco\n",
    "ecoteco_df = valid_event_df[\"Ecoteco\"].copy()\n",
    "ecoteco_df[\"Reported Release Rate (kg/h)\"] = np.nan\n",
    "ecoteco_df[\"EmissionRateUpper\"] = np.nan\n",
    "ecoteco_df[\"EmissionRateLower\"] = np.nan\n",
    "\n",
    "# For Kuva\n",
    "kuva_df = valid_event_df[\"Kuva\"].copy()\n",
    "kuva_df[\"Reported Release Rate (kg/h)\"] = np.nan\n",
    "kuva_df[\"EmissionRateUpper\"] = np.nan\n",
    "kuva_df[\"EmissionRateLower\"] = np.nan\n",
    "\n",
    "\n",
    "# 2. Definition of Events:\n",
    "\n",
    "#     1) All sensors exclude SOOFIE\n",
    "#         1) Positive Events: Release rate > 0 kg/hr or labeled as N/A\n",
    "#         2) Negative Events: Release rate = 0 kg/hr\n",
    "#         3) Events between two consecutive events are classified as negative \n",
    "#     2) Only for SOOFIE\n",
    "#         1) Positive Events: Release Rate > 0 kg/hr\n",
    "#         2) Negative Events: Release Rate = 0 kg/hr\n",
    "#         3) N/A Events: Release Rate is N/A\n",
    "\n",
    "canary_df = valid_event_df[\"Canary\"].copy()\n",
    "qube_df = valid_event_df[\"Qube\"].copy()\n",
    "sensirion_df = valid_event_df[\"Sensirion\"].copy()\n",
    "oiler_df = valid_event_df[\"Oiler\"].copy()\n",
    "soofie_df = valid_event_df[\"Soofie\"].copy()\n",
    "\n",
    "\n",
    "def classfy_for_sensors(df, sensor_name, offline_time=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: pd.DataFrame, data of sensors exclude soofie\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame, classified data\n",
    "    \"\"\"\n",
    "\n",
    "    if sensor_name != \"Soofie\":\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"] > 0, \"ReportLabel\"] = \"P\"\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"].isna(), \"ReportLabel\"] = \"P\"\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"] == 0, \"ReportLabel\"] = \"N\"\n",
    "        df.sort_values(by=\"EmissionStartDateTime\", inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"] > 0, \"ReportLabel\"] = \"P\"\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"] == 0, \"ReportLabel\"] = \"N\"\n",
    "        df.loc[df[\"Reported Release Rate (kg/h)\"].isna(), \"ReportLabel\"] = str(\"NA\")\n",
    "        df.sort_values(by=\"EmissionStartDateTime\", inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if offline_time is not None:\n",
    "        for i in range(offline_time.shape[0]):\n",
    "            start = offline_time.loc[i, \"OfflineReportStartDateTime\"]\n",
    "            end = offline_time.loc[i, \"OfflineReportEndDateTime\"]\n",
    "            idx = df.shape[0]\n",
    "            df.loc[idx] = (\n",
    "                {\n",
    "                    \"EmissionStartDateTime\": start,\n",
    "                    \"EmissionEndDateTime\": end,\n",
    "                    \"Reported Release Rate (kg/h)\": pd.NA,\n",
    "                    \"EmissionRateUpper\": pd.NA,\n",
    "                    \"EmissionRateLower\": pd.NA,\n",
    "                    \"ReportLabel\": str(\"NA\"),\n",
    "                }\n",
    "            )\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # split events which start and end in different days\n",
    "    print(sensor_name)\n",
    "    for i in range(df.shape[0]):\n",
    "        if df[\"EmissionStartDateTime\"].dt.date[i] == df[\"EmissionEndDateTime\"].dt.date[i]:\n",
    "            continue\n",
    "        else:\n",
    "            df.loc[df.shape[0], :] = df.loc[i, :].copy()\n",
    "            df.loc[df.shape[0] - 1, \"EmissionStartDateTime\"] = pd.to_datetime(str(df[\"EmissionEndDateTime\"].dt.date[i]) + \" 00:00:00\")\n",
    "            df.loc[i, \"EmissionEndDateTime\"] = pd.to_datetime(str(df[\"EmissionStartDateTime\"].dt.date[i]) + \" 23:59:59\")\n",
    "\n",
    "    df.sort_values(by=\"EmissionStartDateTime\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # for every dates, if there is no event, then add 00:00:00~23:59:59 as a event with label \"N\"\n",
    "    for date in pd.date_range(start=pd.to_datetime(\"2022-10-10\"), end=pd.to_datetime(\"2022-11-30\"), freq=\"D\"):\n",
    "        tmp_df = df.loc[df[\"EmissionStartDateTime\"].dt.date == date, :]\n",
    "        if tmp_df.shape[0] == 0:\n",
    "            df.loc[df.shape[0]] = [date, date + pd.Timedelta(1, \"d\") - pd.Timedelta(1, \"s\"), 0, 0, 0, \"N\"]\n",
    "            continue\n",
    "\n",
    "        # if the start of first event is not 00:00:00, then add 00:00:00~start_time as a event with label \"N\"\n",
    "        if tmp_df.loc[tmp_df.index[0], \"EmissionStartDateTime\"] != pd.to_datetime(str(date) + \" 00:00:00\"):\n",
    "            df.loc[df.shape[0]] = [date, tmp_df.loc[tmp_df.index[0], \"EmissionStartDateTime\"] - pd.Timedelta(1, \"s\"), 0, 0, 0, \"N\"]\n",
    "        \n",
    "        # if the end of last event is not 23:59:59, then add end_time~23:59:59 as a event with label \"N\"\n",
    "        if tmp_df.loc[tmp_df.index[-1], \"EmissionEndDateTime\"] != pd.to_datetime(str(date) + \" 23:59:59\"):\n",
    "            df.loc[df.shape[0]] = [tmp_df.loc[tmp_df.index[-1], \"EmissionEndDateTime\"] + pd.Timedelta(1, \"s\"), \n",
    "                                   pd.to_datetime(str(date) + \" 23:59:59\"), 0, 0, 0, \"N\"]\n",
    "            \n",
    "        # for every two consecutive events, if the start of second event is not equal to the end of first event, then add a event with label \"N\"\n",
    "        for i in range(tmp_df.shape[0] - 1):\n",
    "            if tmp_df.loc[tmp_df.index[i], \"EmissionEndDateTime\"] != tmp_df.loc[tmp_df.index[i+1], \"EmissionStartDateTime\"]:\n",
    "                df.loc[df.shape[0]] = [tmp_df.loc[tmp_df.index[i], \"EmissionEndDateTime\"] + pd.Timedelta(1, \"s\"), \n",
    "                                       tmp_df.loc[tmp_df.index[i+1], \"EmissionStartDateTime\"] - pd.Timedelta(1, \"s\"), 0, 0, 0, \"N\"]\n",
    "    df.sort_values(by=\"EmissionStartDateTime\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "andium_df = classfy_for_sensors(andium_df, \"Andium\")\n",
    "ecoteco_df = classfy_for_sensors(ecoteco_df, \"Ecoteco\")\n",
    "kuva_df = classfy_for_sensors(kuva_df, \"Kuva\", kuva_offline_time)\n",
    "oiler_df = classfy_for_sensors(oiler_df, \"Oiler\", oiler_offline_time)\n",
    "canary_df = classfy_for_sensors(canary_df, \"Canary\")\n",
    "qube_df = classfy_for_sensors(qube_df, \"Qube\")\n",
    "sensirion_df = classfy_for_sensors(sensirion_df, \"Sensirion\")\n",
    "soofie_df = classfy_for_sensors(soofie_df, \"Soofie\")\n",
    "\n",
    "andium_df[\"ReportDate\"] = andium_df[\"EmissionStartDateTime\"].dt.date\n",
    "ecoteco_df[\"ReportDate\"] = ecoteco_df[\"EmissionStartDateTime\"].dt.date\n",
    "kuva_df[\"ReportDate\"] = kuva_df[\"EmissionStartDateTime\"].dt.date\n",
    "oiler_df[\"ReportDate\"] = oiler_df[\"EmissionStartDateTime\"].dt.date\n",
    "canary_df[\"ReportDate\"] = canary_df[\"EmissionStartDateTime\"].dt.date\n",
    "qube_df[\"ReportDate\"] = qube_df[\"EmissionStartDateTime\"].dt.date\n",
    "sensirion_df[\"ReportDate\"] = sensirion_df[\"EmissionStartDateTime\"].dt.date\n",
    "soofie_df[\"ReportDate\"] = soofie_df[\"EmissionStartDateTime\"].dt.date\n",
    "\n",
    "# 3. Event filtering:\n",
    "#     1) Canary: Short Stack Head Filtering, Exclude events that are not within the dates of October 31, 2022, and November 15 to November 30, 2022.\n",
    "#     2) Kuva: Exclude events overlapping with Offline Report Dates\n",
    "#     3) Oiler: Exclude events overlapping with Offline Report Dates\n",
    "#     4) SOOFIE: Exclude events falling within the period of November 7, 2022, 22:30:00 to November 14, 2022, 16:30:00\n",
    "\n",
    "# For Canary, only use data in short stack head\n",
    "canary_df = canary_df[(canary_df[\"ReportDate\"] == pd.to_datetime(\"2022-10-31\").date()) | \n",
    "                      ((canary_df[\"ReportDate\"] >= pd.to_datetime(\"2022-11-15\").date()) & (canary_df[\"ReportDate\"] <= pd.to_datetime(\"2022-11-30\").date()))]\n",
    "\n",
    "# For Soofie\n",
    "soofie_df = soofie_df[~((soofie_df[\"EmissionStartDateTime\"] >= pd.to_datetime(\"2022-11-07 22:30:00\")) & (soofie_df[\"EmissionEndDateTime\"] <= pd.to_datetime(\"2022-11-14 16:30:00\")))]\n",
    "\n",
    "\n",
    "# 4. For each sensor, filter out all events that do not overlap with the sensor's Online Date\n",
    "# For Andium\n",
    "andium_df = andium_df[(andium_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Andium\", \"Testing Beginning Date\"].values[0]) & \n",
    "                      (andium_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Andium\", \"Testing Ending Date\"].values[0])]\n",
    "# For Ecoteco\n",
    "ecoteco_df = ecoteco_df[(ecoteco_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Ecoteco\", \"Testing Beginning Date\"].values[0]) & \n",
    "                        (ecoteco_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Ecoteco\", \"Testing Ending Date\"].values[0])]\n",
    "# For Kuva\n",
    "kuva_df = kuva_df[(kuva_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Kuva\", \"Testing Beginning Date\"].values[0]) & \n",
    "                  (kuva_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Kuva\", \"Testing Ending Date\"].values[0])]\n",
    "# For Oiler\n",
    "oiler_df = oiler_df[(oiler_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Oiler\", \"Testing Beginning Date\"].values[0]) & \n",
    "                    (oiler_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Oiler\", \"Testing Ending Date\"].values[0])]\n",
    "# For Canary\n",
    "canary_df = canary_df[(canary_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Canary\", \"Testing Beginning Date\"].values[0]) & \n",
    "                      (canary_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Canary\", \"Testing Ending Date\"].values[0])]\n",
    "# For Qube\n",
    "qube_df = qube_df[(qube_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Qube\", \"Testing Beginning Date\"].values[0]) & \n",
    "                  (qube_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Qube\", \"Testing Ending Date\"].values[0])]\n",
    "# For Sensirion\n",
    "sensirion_df = sensirion_df[(sensirion_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Sensirion\", \"Testing Beginning Date\"].values[0]) & \n",
    "                            (sensirion_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Sensirion\", \"Testing Ending Date\"].values[0])]\n",
    "# For Soofie\n",
    "soofie_df = soofie_df[(soofie_df[\"EmissionStartDateTime\"] >= online_date.loc[online_date[\"Sensor\"] == \"Soofie\", \"Testing Beginning Date\"].values[0]) & \n",
    "                      (soofie_df[\"EmissionEndDateTime\"] <= online_date.loc[online_date[\"Sensor\"] == \"Soofie\", \"Testing Ending Date\"].values[0])]\n",
    "\n",
    "\n",
    "# 5. Event Removal: \n",
    "#     - Exclude reports on: 2022-10-14, 2022-10-18 to 2022-10-22\n",
    "#     - Reason: Stanford internal testing period\n",
    "\n",
    "\n",
    "# For Andium\n",
    "andium_df = andium_df[(andium_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                        ((andium_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (andium_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Ecoteco\n",
    "ecoteco_df = ecoteco_df[(ecoteco_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                        ((ecoteco_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (ecoteco_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Kuva\n",
    "kuva_df = kuva_df[(kuva_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                  ((kuva_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (kuva_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Oiler\n",
    "oiler_df = oiler_df[(oiler_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                    ((oiler_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (oiler_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Canary\n",
    "canary_df = canary_df[(canary_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                      ((canary_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (canary_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Qube\n",
    "qube_df = qube_df[(qube_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                  ((qube_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (qube_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Sensirion\n",
    "sensirion_df = sensirion_df[(sensirion_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                            ((sensirion_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (sensirion_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n",
    "# For Soofie\n",
    "soofie_df = soofie_df[(soofie_df[\"EmissionStartDateTime\"].dt.date != pd.to_datetime(\"2022-10-14\").date()) &\n",
    "                      ((soofie_df[\"EmissionStartDateTime\"].dt.date < pd.to_datetime(\"2022-10-18\").date()) | (soofie_df[\"EmissionStartDateTime\"].dt.date > pd.to_datetime(\"2022-10-22\").date()))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Save Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Andium\n",
    "save_path = PurePath('../../assets/sensor_data/Andium_validdata.csv')\n",
    "andium_df.to_csv(save_path, index=None)\n",
    "\n",
    "# For Ecoteco\n",
    "save_path = PurePath('../../assets/sensor_data/Ecoteco_validdata.csv')\n",
    "ecoteco_df.to_csv(save_path, index=None)\n",
    "\n",
    "# For Kuva\n",
    "save_path = PurePath('../../assets/sensor_data/Kuva_validdata.csv')\n",
    "kuva_df.to_csv(save_path, index=None)\n",
    "\n",
    "\n",
    "# For Oiler\n",
    "save_path = PurePath('../../assets/sensor_data/Oiler_validdata.csv')\n",
    "oiler_df.to_csv(save_path, index=None)\n",
    "\n",
    "# For Canary\n",
    "save_path = PurePath('../../assets/sensor_data/Canary_validdata.csv')\n",
    "canary_df.to_csv(save_path, index=None)\n",
    "\n",
    "\n",
    "# For Qube\n",
    "save_path = PurePath('../../assets/sensor_data/Qube_validdata.csv')\n",
    "qube_df.to_csv(save_path, index=None)\n",
    "\n",
    "# For Sensirion\n",
    "save_path = PurePath('../../assets/sensor_data/Sensirion_validdata.csv')\n",
    "sensirion_df.to_csv(save_path, index=None)\n",
    "\n",
    "# For Soofie\n",
    "save_path = PurePath('../../assets/sensor_data/Soofie_validdata.csv')\n",
    "soofie_df.to_csv(save_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
